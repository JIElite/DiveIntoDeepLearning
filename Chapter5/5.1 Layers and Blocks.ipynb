{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359d910c-dce9-471a-bcee-01b32f4fcc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0578, -0.0836, -0.3641,  0.2494, -0.0491, -0.0089,  0.1808, -0.2134,\n",
       "         -0.0831, -0.0790],\n",
       "        [ 0.0251, -0.0319, -0.2416,  0.4722,  0.0154, -0.0632,  0.0998, -0.2911,\n",
       "         -0.2112, -0.1409]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e2bcf4-ed86-4138-9aa7-4c8687154350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # Declare a layer with model parameters. Here, we declare two fully\n",
    "    # connected layers\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the `MLP` parent class `Module` to perform\n",
    "        # the necessary initialization. In this way, other function arguments\n",
    "        # can also be specified during class instantiation, such as the model\n",
    "        # parameters, `params` (to be described later)\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # Hidden layer\n",
    "        self.out = nn.Linear(256, 10)  # Output layer\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input `X`\n",
    "    def forward(self, X):\n",
    "        # Note here we use the funtional version of ReLU defined in the\n",
    "        # nn.functional module.\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7572b184-a56b-428c-9e95-c62cc31c56e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2422,  0.2483, -0.0187,  0.0448,  0.0763, -0.2808,  0.1323, -0.1429,\n",
       "          0.0937, -0.2280],\n",
       "        [-0.0561,  0.2975, -0.0573,  0.0611,  0.1569, -0.2069, -0.0191, -0.2504,\n",
       "          0.0845, -0.2010]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa265d00-655c-41ee-8eca-4d0cff6571c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # Here, `module` is an instance of a `Module` subclass. We save it\n",
    "            # in the member variable `_modules` of the `Module` class, and its\n",
    "            # type is OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict guarantees that members will be traversed in the order\n",
    "        # they were added\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b47a251-69bd-4585-b090-2f190e47bbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: tensor([[ 0.1185,  0.0720, -0.2303,  0.1036,  0.0335,  0.0879, -0.0021, -0.2081,\n",
      "         -0.0555,  0.0891],\n",
      "        [ 0.1720,  0.1440, -0.1750,  0.0897,  0.0380,  0.1066,  0.0493, -0.2080,\n",
      "         -0.0420,  0.1101]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.0457, -0.1934, -0.1552,  ..., -0.0041,  0.2154, -0.0730],\n",
       "                      [ 0.0066,  0.0865, -0.1483,  ...,  0.1223,  0.0777, -0.0900],\n",
       "                      [-0.0904, -0.1296, -0.1531,  ..., -0.1750, -0.1695,  0.0273],\n",
       "                      ...,\n",
       "                      [ 0.1786,  0.1122, -0.1993,  ...,  0.1870, -0.1312, -0.2168],\n",
       "                      [ 0.2013,  0.2107,  0.2215,  ..., -0.0210, -0.0707, -0.0265],\n",
       "                      [ 0.0755,  0.0598, -0.1234,  ...,  0.1085,  0.1218,  0.0070]])),\n",
       "             ('0.bias',\n",
       "              tensor([-9.0977e-02,  3.2079e-02, -4.0121e-02,  1.4035e-01,  2.2254e-01,\n",
       "                      -1.2892e-01, -6.8983e-02,  1.9484e-01, -2.2253e-01, -1.2277e-01,\n",
       "                      -7.6194e-02, -1.9712e-01, -1.4121e-01,  3.6746e-02,  1.1090e-01,\n",
       "                       1.1370e-02, -1.5556e-01, -2.0861e-01, -2.1543e-01, -4.5025e-02,\n",
       "                      -2.3293e-02, -1.4584e-01,  1.6882e-01,  2.1979e-01,  1.1667e-01,\n",
       "                      -8.3702e-02,  3.2990e-02,  5.0170e-02, -2.1771e-01,  1.4476e-01,\n",
       "                       1.3525e-01,  1.7044e-01, -2.1055e-01, -6.3055e-02, -2.2813e-02,\n",
       "                      -1.8948e-02, -1.3277e-01, -5.5505e-02,  1.4157e-01, -1.8369e-01,\n",
       "                      -1.3907e-01,  3.1188e-02,  8.7610e-02, -5.6987e-02, -5.4670e-02,\n",
       "                      -1.9540e-01, -8.9036e-02,  1.8227e-01,  1.2398e-01,  1.4090e-01,\n",
       "                       1.0284e-01, -1.2884e-01, -2.0228e-01,  2.0023e-01,  9.6458e-02,\n",
       "                       2.8125e-02, -6.8088e-02,  9.1540e-02, -7.8409e-02,  1.6240e-01,\n",
       "                      -1.7805e-02,  1.2951e-01, -1.9480e-01,  1.2195e-01, -1.5412e-01,\n",
       "                      -1.0793e-01,  8.0944e-04,  4.5122e-02,  1.7380e-01,  6.1739e-02,\n",
       "                      -1.7693e-01, -1.1313e-01,  1.4775e-01,  2.9411e-02,  1.2313e-02,\n",
       "                       2.9836e-02,  1.9679e-01, -1.6097e-01, -9.5343e-02, -5.4711e-02,\n",
       "                       2.2094e-01,  1.7143e-01, -1.6240e-01, -2.1135e-01, -1.8709e-01,\n",
       "                      -6.8402e-03,  4.8885e-02,  5.4210e-02,  1.6737e-01,  1.7744e-01,\n",
       "                      -1.2001e-01,  6.9466e-03, -2.0029e-01,  1.8928e-01,  9.3915e-03,\n",
       "                      -1.9710e-01,  7.0768e-02, -4.2005e-04,  8.4051e-02, -1.2083e-01,\n",
       "                      -8.0566e-02, -5.2761e-02,  9.3927e-02, -2.0120e-01,  2.1510e-02,\n",
       "                       1.5935e-01,  2.2060e-01, -2.1888e-01, -1.5000e-01, -4.8106e-02,\n",
       "                       1.8077e-01, -1.5566e-01,  3.8882e-02,  1.5213e-04, -1.0368e-02,\n",
       "                      -1.9527e-01, -2.1232e-01, -1.5209e-01,  1.3777e-01,  8.7407e-02,\n",
       "                       1.0596e-02, -1.1909e-02,  1.6587e-01,  1.6616e-01,  4.6473e-02,\n",
       "                       1.3525e-01,  4.4945e-02,  1.8996e-02,  1.9117e-01, -6.6925e-02,\n",
       "                       1.1078e-01,  1.1842e-01,  8.8889e-02, -1.0430e-01, -1.9019e-01,\n",
       "                       1.7292e-01,  5.6120e-02, -1.5611e-01,  2.2094e-01,  2.4324e-02,\n",
       "                      -1.7038e-01, -1.3702e-01,  1.1683e-01, -6.6797e-02, -7.3459e-02,\n",
       "                      -3.4490e-02,  1.8684e-01, -1.5680e-01,  7.7531e-03,  1.5024e-01,\n",
       "                      -1.1685e-01,  7.4709e-02, -1.4758e-01, -9.0901e-02,  7.9037e-02,\n",
       "                      -5.7700e-02, -1.8744e-02,  1.3577e-01,  1.8485e-02, -7.8677e-02,\n",
       "                      -2.0080e-01,  5.4879e-02,  4.2914e-02, -1.2101e-01, -2.1825e-01,\n",
       "                       1.0473e-01,  1.0917e-01,  1.9987e-01, -2.1891e-01,  1.1069e-02,\n",
       "                       7.4265e-02, -2.0139e-01,  2.0793e-01,  1.8705e-01, -1.9705e-01,\n",
       "                       1.1327e-01, -7.5346e-02,  9.9010e-02, -1.1517e-01, -1.8824e-01,\n",
       "                      -1.5732e-01, -1.5756e-01, -2.0148e-01,  1.5354e-01,  1.6631e-01,\n",
       "                       1.6412e-01, -1.3541e-01,  5.7933e-02,  1.7583e-01,  2.5266e-02,\n",
       "                      -4.7793e-02, -2.4928e-02, -1.6205e-01,  1.3382e-01, -1.0888e-01,\n",
       "                       1.5938e-01,  1.1741e-01, -5.6289e-02, -1.5730e-01,  1.6060e-01,\n",
       "                      -9.7363e-02,  1.4000e-01,  2.6432e-02,  1.4160e-01,  2.1618e-01,\n",
       "                      -2.0162e-01, -3.0588e-02, -2.1483e-01,  1.8223e-01,  1.5933e-01,\n",
       "                       1.9282e-01,  2.0952e-01, -6.7979e-02,  9.5831e-02,  8.1924e-02,\n",
       "                      -6.4524e-02, -1.0364e-01,  7.6125e-02, -1.7844e-01, -1.0138e-01,\n",
       "                      -9.1476e-02, -1.1603e-01, -2.8220e-02,  9.2785e-02, -1.8820e-01,\n",
       "                       1.6326e-01, -4.6397e-02,  1.6513e-01, -2.1921e-01,  8.9026e-02,\n",
       "                      -1.6435e-01, -1.8532e-01, -1.1333e-01,  1.9321e-01, -1.0916e-01,\n",
       "                      -1.4426e-02,  2.0524e-01,  7.5490e-02, -1.5159e-01, -3.1428e-02,\n",
       "                       1.3067e-02, -3.2359e-02,  7.1678e-02,  2.6687e-02,  1.3029e-02,\n",
       "                      -1.3050e-01, -1.7308e-01, -1.4736e-01, -1.2818e-02,  2.6706e-02,\n",
       "                      -7.6815e-02,  1.2046e-01,  7.2890e-02, -1.4267e-01,  5.0355e-02,\n",
       "                       2.1355e-01])),\n",
       "             ('2.weight',\n",
       "              tensor([[ 0.0544, -0.0412, -0.0107,  ..., -0.0201,  0.0270,  0.0273],\n",
       "                      [-0.0339, -0.0518,  0.0616,  ...,  0.0403,  0.0572,  0.0048],\n",
       "                      [-0.0171,  0.0286,  0.0157,  ...,  0.0186,  0.0469,  0.0546],\n",
       "                      ...,\n",
       "                      [-0.0382,  0.0359, -0.0427,  ...,  0.0528,  0.0377,  0.0087],\n",
       "                      [-0.0508, -0.0104, -0.0380,  ...,  0.0062,  0.0585,  0.0188],\n",
       "                      [ 0.0454, -0.0481,  0.0123,  ..., -0.0414,  0.0517,  0.0580]])),\n",
       "             ('2.bias',\n",
       "              tensor([-0.0492,  0.0092, -0.0607, -0.0185, -0.0499, -0.0243, -0.0569,  0.0176,\n",
       "                      -0.0546, -0.0557]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "result = net(X)\n",
    "print('Result:', result)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dbf928d-78ad-4dd5-8415-5e1327d30233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0457, -0.1934, -0.1552,  ..., -0.0041,  0.2154, -0.0730],\n",
      "        [ 0.0066,  0.0865, -0.1483,  ...,  0.1223,  0.0777, -0.0900],\n",
      "        [-0.0904, -0.1296, -0.1531,  ..., -0.1750, -0.1695,  0.0273],\n",
      "        ...,\n",
      "        [ 0.1786,  0.1122, -0.1993,  ...,  0.1870, -0.1312, -0.2168],\n",
      "        [ 0.2013,  0.2107,  0.2215,  ..., -0.0210, -0.0707, -0.0265],\n",
      "        [ 0.0755,  0.0598, -0.1234,  ...,  0.1085,  0.1218,  0.0070]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-9.0977e-02,  3.2079e-02, -4.0121e-02,  1.4035e-01,  2.2254e-01,\n",
      "        -1.2892e-01, -6.8983e-02,  1.9484e-01, -2.2253e-01, -1.2277e-01,\n",
      "        -7.6194e-02, -1.9712e-01, -1.4121e-01,  3.6746e-02,  1.1090e-01,\n",
      "         1.1370e-02, -1.5556e-01, -2.0861e-01, -2.1543e-01, -4.5025e-02,\n",
      "        -2.3293e-02, -1.4584e-01,  1.6882e-01,  2.1979e-01,  1.1667e-01,\n",
      "        -8.3702e-02,  3.2990e-02,  5.0170e-02, -2.1771e-01,  1.4476e-01,\n",
      "         1.3525e-01,  1.7044e-01, -2.1055e-01, -6.3055e-02, -2.2813e-02,\n",
      "        -1.8948e-02, -1.3277e-01, -5.5505e-02,  1.4157e-01, -1.8369e-01,\n",
      "        -1.3907e-01,  3.1188e-02,  8.7610e-02, -5.6987e-02, -5.4670e-02,\n",
      "        -1.9540e-01, -8.9036e-02,  1.8227e-01,  1.2398e-01,  1.4090e-01,\n",
      "         1.0284e-01, -1.2884e-01, -2.0228e-01,  2.0023e-01,  9.6458e-02,\n",
      "         2.8125e-02, -6.8088e-02,  9.1540e-02, -7.8409e-02,  1.6240e-01,\n",
      "        -1.7805e-02,  1.2951e-01, -1.9480e-01,  1.2195e-01, -1.5412e-01,\n",
      "        -1.0793e-01,  8.0944e-04,  4.5122e-02,  1.7380e-01,  6.1739e-02,\n",
      "        -1.7693e-01, -1.1313e-01,  1.4775e-01,  2.9411e-02,  1.2313e-02,\n",
      "         2.9836e-02,  1.9679e-01, -1.6097e-01, -9.5343e-02, -5.4711e-02,\n",
      "         2.2094e-01,  1.7143e-01, -1.6240e-01, -2.1135e-01, -1.8709e-01,\n",
      "        -6.8402e-03,  4.8885e-02,  5.4210e-02,  1.6737e-01,  1.7744e-01,\n",
      "        -1.2001e-01,  6.9466e-03, -2.0029e-01,  1.8928e-01,  9.3915e-03,\n",
      "        -1.9710e-01,  7.0768e-02, -4.2005e-04,  8.4051e-02, -1.2083e-01,\n",
      "        -8.0566e-02, -5.2761e-02,  9.3927e-02, -2.0120e-01,  2.1510e-02,\n",
      "         1.5935e-01,  2.2060e-01, -2.1888e-01, -1.5000e-01, -4.8106e-02,\n",
      "         1.8077e-01, -1.5566e-01,  3.8882e-02,  1.5213e-04, -1.0368e-02,\n",
      "        -1.9527e-01, -2.1232e-01, -1.5209e-01,  1.3777e-01,  8.7407e-02,\n",
      "         1.0596e-02, -1.1909e-02,  1.6587e-01,  1.6616e-01,  4.6473e-02,\n",
      "         1.3525e-01,  4.4945e-02,  1.8996e-02,  1.9117e-01, -6.6925e-02,\n",
      "         1.1078e-01,  1.1842e-01,  8.8889e-02, -1.0430e-01, -1.9019e-01,\n",
      "         1.7292e-01,  5.6120e-02, -1.5611e-01,  2.2094e-01,  2.4324e-02,\n",
      "        -1.7038e-01, -1.3702e-01,  1.1683e-01, -6.6797e-02, -7.3459e-02,\n",
      "        -3.4490e-02,  1.8684e-01, -1.5680e-01,  7.7531e-03,  1.5024e-01,\n",
      "        -1.1685e-01,  7.4709e-02, -1.4758e-01, -9.0901e-02,  7.9037e-02,\n",
      "        -5.7700e-02, -1.8744e-02,  1.3577e-01,  1.8485e-02, -7.8677e-02,\n",
      "        -2.0080e-01,  5.4879e-02,  4.2914e-02, -1.2101e-01, -2.1825e-01,\n",
      "         1.0473e-01,  1.0917e-01,  1.9987e-01, -2.1891e-01,  1.1069e-02,\n",
      "         7.4265e-02, -2.0139e-01,  2.0793e-01,  1.8705e-01, -1.9705e-01,\n",
      "         1.1327e-01, -7.5346e-02,  9.9010e-02, -1.1517e-01, -1.8824e-01,\n",
      "        -1.5732e-01, -1.5756e-01, -2.0148e-01,  1.5354e-01,  1.6631e-01,\n",
      "         1.6412e-01, -1.3541e-01,  5.7933e-02,  1.7583e-01,  2.5266e-02,\n",
      "        -4.7793e-02, -2.4928e-02, -1.6205e-01,  1.3382e-01, -1.0888e-01,\n",
      "         1.5938e-01,  1.1741e-01, -5.6289e-02, -1.5730e-01,  1.6060e-01,\n",
      "        -9.7363e-02,  1.4000e-01,  2.6432e-02,  1.4160e-01,  2.1618e-01,\n",
      "        -2.0162e-01, -3.0588e-02, -2.1483e-01,  1.8223e-01,  1.5933e-01,\n",
      "         1.9282e-01,  2.0952e-01, -6.7979e-02,  9.5831e-02,  8.1924e-02,\n",
      "        -6.4524e-02, -1.0364e-01,  7.6125e-02, -1.7844e-01, -1.0138e-01,\n",
      "        -9.1476e-02, -1.1603e-01, -2.8220e-02,  9.2785e-02, -1.8820e-01,\n",
      "         1.6326e-01, -4.6397e-02,  1.6513e-01, -2.1921e-01,  8.9026e-02,\n",
      "        -1.6435e-01, -1.8532e-01, -1.1333e-01,  1.9321e-01, -1.0916e-01,\n",
      "        -1.4426e-02,  2.0524e-01,  7.5490e-02, -1.5159e-01, -3.1428e-02,\n",
      "         1.3067e-02, -3.2359e-02,  7.1678e-02,  2.6687e-02,  1.3029e-02,\n",
      "        -1.3050e-01, -1.7308e-01, -1.4736e-01, -1.2818e-02,  2.6706e-02,\n",
      "        -7.6815e-02,  1.2046e-01,  7.2890e-02, -1.4267e-01,  5.0355e-02,\n",
      "         2.1355e-01], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0544, -0.0412, -0.0107,  ..., -0.0201,  0.0270,  0.0273],\n",
      "        [-0.0339, -0.0518,  0.0616,  ...,  0.0403,  0.0572,  0.0048],\n",
      "        [-0.0171,  0.0286,  0.0157,  ...,  0.0186,  0.0469,  0.0546],\n",
      "        ...,\n",
      "        [-0.0382,  0.0359, -0.0427,  ...,  0.0528,  0.0377,  0.0087],\n",
      "        [-0.0508, -0.0104, -0.0380,  ...,  0.0062,  0.0585,  0.0188],\n",
      "        [ 0.0454, -0.0481,  0.0123,  ..., -0.0414,  0.0517,  0.0580]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0492,  0.0092, -0.0607, -0.0185, -0.0499, -0.0243, -0.0569,  0.0176,\n",
      "        -0.0546, -0.0557], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "g = net.parameters()\n",
    "for p in g:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f263bb-2152-42fa-be01-88ea10391692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # Use the created constant parameters, as well as the `relu` and `mm`\n",
    "        # functions\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # Reuse the fully-connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully-connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd16b82f-a67a-4210-8efc-885716bb298b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0366, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9d5bb5-0dac-4365-97b5-b10ec3889c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2177, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d18c2f0-8fd8-4159-bc66-e30822fa3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequentialExer(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.modules = []\n",
    "        for idx, module in enumerate(args):\n",
    "                self.modules.append(module)\n",
    "                \n",
    "    def forward(self, X):\n",
    "        for block in self.modules:\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efdb3245-cdb8-4ad4-9d4a-fe713b623c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Result: tensor([[-0.0223],\n",
      "        [ 0.0319]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequentialExer(nn.Linear(20, 10), nn.ReLU(), nn.Linear(10, 1))\n",
    "result = net(X)\n",
    "print('Inference Result:', result)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c603486-8acd-4d5c-9b8f-6bf3da56af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = net.parameters()\n",
    "for p in g:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f380f28d-a743-451e-95e2-ea847065b44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x7f5aeea2cdd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bf48b95-7e49-4dd6-92df-915dcde2b810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of MySequential(\n",
       "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5d9a074-84e0-4b39-a5c6-45c2866f2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, ins, outs, non_linear=False):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(ins, outs)\n",
    "        self.non_linear = non_linear\n",
    "        if self.non_linear:\n",
    "            self.non_linear_layer = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        if self.non_linear:\n",
    "            x = self.non_linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2d2cee3-e5c5-46c0-833b-83fe6a940612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_blocks(ins, outs, n, non_linear=False):\n",
    "    modules = nn.Sequential()\n",
    "    for i in range(n-1):\n",
    "        modules.add_module(str(i), Block(ins, ins, non_linear=non_linear))\n",
    "    modules.add_module(str(n), Block(ins, outs))\n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5f64ee3-8b8e-4f45-a93b-3f603da45427",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = build_blocks(20, 1, 10, non_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f72d9b1-ae00-4ffd-901a-3a7246a16f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Block(\n",
       "    (layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (non_linear_layer): ReLU()\n",
       "  )\n",
       "  (1): Block(\n",
       "    (layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (non_linear_layer): ReLU()\n",
       "  )\n",
       "  (2): Block(\n",
       "    (layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (non_linear_layer): ReLU()\n",
       "  )\n",
       "  (3): Block(\n",
       "    (layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (non_linear_layer): ReLU()\n",
       "  )\n",
       "  (4): Block(\n",
       "    (layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (non_linear_layer): ReLU()\n",
       "  )\n",
       "  (5): Block(\n",
       "    (layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (non_linear_layer): ReLU()\n",
       "  )\n",
       "  (6): Block(\n",
       "    (layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (non_linear_layer): ReLU()\n",
       "  )\n",
       "  (7): Block(\n",
       "    (layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (non_linear_layer): ReLU()\n",
       "  )\n",
       "  (8): Block(\n",
       "    (layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (non_linear_layer): ReLU()\n",
       "  )\n",
       "  (10): Block(\n",
       "    (layer): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a4cc17-9c54-4fbf-b597-e08c83adda2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
