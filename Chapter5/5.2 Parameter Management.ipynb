{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c231af2b-b080-4bad-90e3-2a046511a8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0271],\n",
       "        [0.1779]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d581f385-ea34-4cd8-9318-6f1138539721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.1011,  0.1472, -0.2475, -0.2094,  0.0992, -0.2100, -0.0397, -0.2418]])), ('bias', tensor([0.2059]))])\n"
     ]
    }
   ],
   "source": [
    "# We can access model parameters explicitly\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79937048-6e7e-47a0-8ead-d541b20ae827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2059], requires_grad=True)\n",
      "tensor([0.2059])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea457ee4-167c-4d99-a691-0faf887d5ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The initial gradient value is None\n",
    "net[2].weight.grad == None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4c1574-497a-44f7-9f74-fcaa246f44b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5094a8-590c-4aea-97d4-9e4e497a4992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.4940, -0.2946,  0.2286,  0.0901],\n",
       "                      [ 0.2484, -0.4816,  0.1595, -0.4982],\n",
       "                      [ 0.0220, -0.1336,  0.0839,  0.3234],\n",
       "                      [-0.4448, -0.2009,  0.1759,  0.3437],\n",
       "                      [-0.0507, -0.1753,  0.0928, -0.4410],\n",
       "                      [-0.3591, -0.1541,  0.4844, -0.2122],\n",
       "                      [-0.2660, -0.3265, -0.4585, -0.4336],\n",
       "                      [-0.0945, -0.2040, -0.0870, -0.4333]])),\n",
       "             ('0.bias',\n",
       "              tensor([ 0.4415,  0.3722, -0.4617,  0.2411, -0.1614, -0.0087,  0.0983,  0.2475])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.1011,  0.1472, -0.2475, -0.2094,  0.0992, -0.2100, -0.0397, -0.2418]])),\n",
       "             ('2.bias', tensor([0.2059]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8645248-cb04-46ac-9a61-829c9747e9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4940, -0.2946,  0.2286,  0.0901],\n",
       "        [ 0.2484, -0.4816,  0.1595, -0.4982],\n",
       "        [ 0.0220, -0.1336,  0.0839,  0.3234],\n",
       "        [-0.4448, -0.2009,  0.1759,  0.3437],\n",
       "        [-0.0507, -0.1753,  0.0928, -0.4410],\n",
       "        [-0.3591, -0.1541,  0.4844, -0.2122],\n",
       "        [-0.2660, -0.3265, -0.4585, -0.4336],\n",
       "        [-0.0945, -0.2040, -0.0870, -0.4333]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['0.weight'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a197ab-7b10-479d-b4d9-7af152a2e976",
   "metadata": {},
   "source": [
    "## 5.2.1.3. Collecting Parameters from Nested Blocks\n",
    "Let us see how the parameter naming conventions work if we nest multiple blocks inside each other. For that we first define a function that produces blocks (a block factory, so to speak) and then combine these inside yet larger blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fa4653b-f74b-4166-9925-b1e8f138508b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2672],\n",
       "        [0.2672]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4),\n",
    "                         nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # Nested here\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76e2b2e9-6e5a-4cc3-b620-492d5f589078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (block 0): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 1): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 3): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7df86f09-b50f-4228-acc4-82c863e4704e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2207, -0.1982,  0.2267,  0.0655,  0.2137,  0.4453, -0.0392,  0.0495])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access nested parameters\n",
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de63e2-4da6-43e7-95c8-b604a91df801",
   "metadata": {},
   "source": [
    "## 5.2.2. Parameter Initialization\n",
    "By default, PyTorch initializes weight and bias matrices uniformly by drawing from a range that is computed according to the input and output dimension. PyTorchâ€™s nn.init module provides a variety of preset initialization methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cb255ae-d469-48d1-b234-f851addd484a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0113, -0.0115, -0.0025, -0.0043]), tensor(0.))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07de85-393b-40da-ab1f-9a4cf48c3934",
   "metadata": {},
   "source": [
    "Question: Why do we use `if type(m) == nn.Linear` rather than `if isinstance(m, nn.Linear)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbdced60-3d6b-4ce2-a722-a775033988f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0113, -0.0115, -0.0025, -0.0043]), tensor(0.))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal_(m):\n",
    "    if isinstance(m, nn.Linear): that they are actually the same object rather than just having the\n",
    "# same value\n",
    "        \n",
    "        # NOTICE: we use in-place function to initialize the parameters\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "torch.manual_seed(0)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d093a-17eb-46fc-b2fa-2d6c22eab286",
   "metadata": {},
   "source": [
    "We can get the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1aa5b31b-bb5f-40f4-ace9-f02bf36fc951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406a6cf-cab6-431c-aec2-fd66be61a068",
   "metadata": {},
   "source": [
    "We can also apply different initializers for certain blocks. For example, below we initialize the first layer with the Xavier initializer and initialize the second layer to a constant value of 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d432589-032f-413d-9f48-ed6863e0163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3136, -0.0255,  0.4522,  0.7030])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "# module.apply(function)\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928aba7-fcf7-471d-ba57-a989e8ff2488",
   "metadata": {},
   "source": [
    "## 5.2.2.2. Custom Initialization\n",
    "Sometimes, the initialization methods we need are not provided by the deep learning framework. In the example below, we define an initializer for any weight parameter  w  using the following strange distribution:\n",
    "\n",
    "$$\n",
    "w \\sim \n",
    "\\begin{cases}\n",
    "    U(5, 10) & \\text{with probability} \\ \\frac{1}{4} \\\\\n",
    "    0 & \\text{with probability} \\ \\frac{1}{2} \\\\\n",
    "    U(-10, -5) & \\text{with probability} \\ \\frac{1}{4}\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5ae411b-227f-4baf-b7fc-4cc6dc0be57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-implemented Customized Initialization\n",
    "def custom_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd272b56-e42f-4f6b-bf5d-1786bdfe71af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Fills the input Tensor with values drawn from the uniform\n",
       "distribution :math:`\\mathcal{U}(a, b)`.\n",
       "\n",
       "Args:\n",
       "    tensor: an n-dimensional `torch.Tensor`\n",
       "    a: the lower bound of the uniform distribution\n",
       "    b: the upper bound of the uniform distribution\n",
       "\n",
       "Examples:\n",
       "    >>> w = torch.empty(3, 5)\n",
       "    >>> nn.init.uniform_(w)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/torch182_py38/lib/python3.8/site-packages/torch/nn/init.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.init.uniform_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d60fe86-1e4c-4a1e-8ca3-c15ecffa1592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-9.3195,  8.8849,  7.6036, -9.9753],\n",
       "        [ 0.0000, -0.0000, -0.0000, -0.0000]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Book-Provided Customized Initialization\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4dca0-d8f3-4eab-8ca2-f217cb1fc34f",
   "metadata": {},
   "source": [
    "## 5.2.3. Tied Parameters\n",
    "Often, we want to share parameters across multiple layers. Let us see how to do this elegantly. In the following we allocate a dense layer and then use its parameters specifically to set those of another layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5490f484-5314-4704-b4ed-c510a721cb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared,\n",
    "                    nn.ReLU(), nn.Linear(8, 1))\n",
    "net(X)\n",
    "\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "net[2].bias.data[7] = 100\n",
    "\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "108ca792-139a-49ae-9c5f-e0380792f851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 1.0000e+02, -1.6142e-01,  1.8523e-01, -1.6355e-01, -1.7413e-01,\n",
       "                       -3.0926e-02, -3.3983e-02, -2.7541e-01],\n",
       "                      [ 2.9475e-01, -1.5596e-01,  1.2541e-01,  3.0751e-01,  1.7830e-01,\n",
       "                        5.0056e-02,  3.0082e-01,  4.7527e-02],\n",
       "                      [-1.6357e-01,  3.3446e-01,  8.3679e-02, -3.4496e-01, -1.0065e-01,\n",
       "                       -2.4083e-01,  3.0998e-01, -5.8388e-02],\n",
       "                      [-3.2227e-01, -2.2226e-02,  2.2204e-01,  9.1853e-02,  1.1180e-01,\n",
       "                        3.2836e-02,  1.3181e-01, -8.6144e-02],\n",
       "                      [-1.4064e-01, -3.3047e-01, -2.6634e-01,  1.5323e-01, -2.0934e-01,\n",
       "                        5.0723e-02,  1.1285e-01,  2.5033e-02],\n",
       "                      [-2.2923e-01,  3.3811e-01, -2.0559e-01,  2.9080e-01, -2.8125e-01,\n",
       "                       -8.5047e-02,  1.9232e-01, -1.4446e-01],\n",
       "                      [ 2.9700e-01, -2.4330e-01, -2.9690e-01, -1.5942e-01,  5.7152e-02,\n",
       "                        3.2554e-01, -1.6879e-01,  1.2644e-01],\n",
       "                      [-8.8638e-02, -7.6672e-02,  2.5997e-01, -2.7400e-01,  3.7552e-02,\n",
       "                        3.3248e-01, -4.8583e-02,  2.7451e-01]])),\n",
       "             ('bias',\n",
       "              tensor([-1.0888e-01,  2.8460e-01, -3.4200e-01, -5.0955e-02, -6.2100e-02,\n",
       "                       1.1458e-01,  1.3871e-01,  1.0000e+02]))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c1196-4ce3-4f3e-ab28-4c93ff2b1747",
   "metadata": {},
   "source": [
    "**Question**: Construct an MLP containing a shared parameter layer and train it. During the training process, observe the model parameters and gradients of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656e84c-6427-430d-9d01-0e054c1b6a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
